{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb0597a",
   "metadata": {},
   "source": [
    "# Deep Learning-Based Name Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29926fe0",
   "metadata": {},
   "source": [
    "What I will do:\n",
    "\n",
    "1. **Input 2 Names**: If we have two names, like \"John\" and \"Mary\". These names are the input data we want to analyze.\n",
    "\n",
    "2. **Tokenization**: Tokenization means breaking down each name into smaller parts, typically words or characters. For our names, tokenization would simply split them into individual characters because names are already small and don't contain spaces.\n",
    "\n",
    "   - Example:\n",
    "     - \"John\" → [\"J\", \"o\", \"h\", \"n\"]\n",
    "     - \"Mary\" → [\"M\", \"a\", \"r\", \"y\"]\n",
    "\n",
    "3. **Word Embedding**: Word embedding is a way to convert these tokens (characters in this case) into meaningful numerical representations (vectors) that capture the context and relationships between words.\n",
    "\n",
    "   - Each character (token) is assigned a unique numerical vector. These vectors have specific values that encode information about the character's meaning or usage.\n",
    "\n",
    "     Example (hypothetical):\n",
    "     - \"J\" → [0.3, -0.1, 0.8]\n",
    "     - \"o\" → [-0.2, 0.5, -0.6]\n",
    "     - \"h\" → [0.7, 0.4, -0.2]\n",
    "     - \"n\" → [0.1, -0.3, 0.6]\n",
    "\n",
    "     - \"M\" → [-0.5, 0.2, -0.7]\n",
    "     - \"a\" → [0.4, -0.6, 0.3]\n",
    "     - \"r\" → [0.2, 0.1, -0.4]\n",
    "     - \"y\" → [-0.3, 0.7, -0.1]\n",
    "\n",
    "4. **Numerical Representation**: Now, each name (\"John\" and \"Mary\") is represented as a sequence of these numerical vectors by combining the vectors of its constituent tokens (characters).\n",
    "\n",
    "   - For \"John\":\n",
    "     - Vector representation of \"John\" = [Vector(\"J\") + Vector(\"o\") + Vector(\"h\") + Vector(\"n\")]\n",
    "     - Example (hypothetical): [0.3, -0.1, 0.8] + [-0.2, 0.5, -0.6] + [0.7, 0.4, -0.2] + [0.1, -0.3, 0.6] = [0.9, 0.5, -0.4]\n",
    "\n",
    "   - For \"Mary\":\n",
    "     - Vector representation of \"Mary\" = [Vector(\"M\") + Vector(\"a\") + Vector(\"r\") + Vector(\"y\")]\n",
    "     - Example (hypothetical): [-0.5, 0.2, -0.7] + [0.4, -0.6, 0.3] + [0.2, 0.1, -0.4] + [-0.3, 0.7, -0.1] = [-0.2, 0.4, -0.9]\n",
    "\n",
    "5. **Dot Product for Similarity**: The dot product is a mathematical operation used to measure the similarity between two vectors. In this case, we can calculate the dot product between the vector representations of \"John\" and \"Mary\" to determine how similar they are based on their numerical representations.\n",
    "\n",
    "   - Dot product formula between two vectors (a and b): a · b = a1 * b1 + a2 * b2 + ... + an * bn (where ai and bi are components of vectors a and b)\n",
    "\n",
    "   - Example (hypothetical):\n",
    "     - Dot product of \"John\" and \"Mary\" = [0.9, 0.5, -0.4] · [-0.2, 0.4, -0.9] \n",
    "     - = (0.9 * -0.2) + (0.5 * 0.4) + (-0.4 * -0.9) \n",
    "     - = -0.18 + 0.2 + 0.36 \n",
    "     - = 0.38\n",
    "\n",
    "   - The resulting value (0.38 in this example) from the dot product indicates the similarity between \"John\" and \"Mary\". A higher value suggests more similarity in their numerical representations, while a lower value suggests less similarity.\n",
    "\n",
    "Therefore, by using tokenization, word embedding, and the dot product, we can mathematically quantify the similarity between two names (\"John\" and \"Mary\") based on their underlying meanings and contexts as represented by numerical vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade1c16",
   "metadata": {},
   "source": [
    "I will use Bert transformer.\n",
    "\n",
    "BertTokenizer:\n",
    "\n",
    "This class is responsible for tokenizing (breaking down) text into individual tokens that can be understood by BERT and other similar models.\n",
    "It handles tasks like splitting words into subwords (sub-tokenization) using the WordPiece algorithm, converting tokens to IDs (numerical representations), and adding special tokens for tasks like classification or question answering.\n",
    "\n",
    "BertModel:\n",
    "\n",
    "This class represents the BERT model itself, which is a deep neural network architecture pre-trained on large text corpora.\n",
    "The BertModel is capable of transforming input text (tokenized sequences) into rich context representations (embeddings) that capture the meaning and context of words within sentences or documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e208d6",
   "metadata": {},
   "source": [
    "Choosing a BERT model:\n",
    "\n",
    "\n",
    "**Word Piece Tokenization**: BERT uses a subword tokenization approach (WordPiece), which means it can handle out-of-vocabulary words and break down complex words into smaller meaningful subunits. This is particularly useful when dealing with abbreviated or truncated names commonly found in transaction descriptions.\n",
    "\n",
    "**Pre-trained Language Model**: BERT is pre-trained on a large corpus of text data, which includes a wide range of language patterns and nuances. This pre-training helps BERT to generalize well across different domains and tasks, including matching names with varying forms.\n",
    "\n",
    "**Transfer Learning Benefits**: Leveraging BERT for name matching involves transfer learning, where the model's pre-trained knowledge is transferred to a specific task (name matching). This often leads to improved performance with less labeled data required for training -- in our case, we don't have any data\n",
    "\n",
    "**State-of-the-art Performance**: BERT has demonstrated state-of-the-art performance on various NLP benchmarks and tasks. It's a widely adopted and well-tested model that can provide strong performance for name matching task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20ac58",
   "metadata": {},
   "source": [
    "### Model Architecture:\n",
    "\n",
    "**Token Embedding Layer**:\n",
    "   - Convert input names and transaction descriptions into token sequences.\n",
    "   - Use a tokenization method that captures subword units and handles variations/abbreviations effectively.\n",
    "\n",
    "**Pre-trained Language Model (BERT)**:\n",
    "   - Fine-tune a pre-trained transformer model (BERT) for the name matching task:\n",
    "     - **Input Representation**: Convert tokenized inputs into contextualized embeddings using BERT's token embedding layer.\n",
    "\n",
    "**Sequence Matching Layer**:\n",
    "\n",
    "   - **BERT-based Approach**:\n",
    "     - Use BERT's output embeddings (token representation) as input to downstream sequence matching layers (fully connected layers, softmax for classification).\n",
    "\n",
    "**Output Layer**:\n",
    "   - Output layer to predict the similarity or matching score between customer names and transaction descriptions:\n",
    "     - **Ranking/Scoring**: Assign a similarity score (cosine similarity) to quantify the match strength between names and transaction descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110564c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c944ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1fd11db4af45af905ca2667998f57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adel\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Adel\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model tokenizer and model\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17dce53",
   "metadata": {},
   "source": [
    "These lines of code initialize a BERT model and tokenizer for tokenizing and encoding text into numerical representations.\n",
    "- `tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')`: Loads a pre-trained BERT tokenizer capable of converting text into tokens understood by BERT.\n",
    "- `model = BertModel.from_pretrained('bert-base-uncased')`: Loads a pre-trained BERT model for processing tokenized text and generating contextual embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b90ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    #Encode text\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=32)\n",
    "    #Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    #Take the first token ([CLS]) embeddings for each sample\n",
    "    embeddings = output.last_hidden_state[:, 0, :]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9983c3d",
   "metadata": {},
   "source": [
    "This `encode` function takes a text input, encodes it using a pre-trained transformer model (like BERT) and a given tokenizer, then extracts embeddings for the text.\n",
    "\n",
    "- It first tokenizes the `text` input using the provided `tokenizer`, ensuring padding, truncation, and a maximum sequence length of 32 tokens.\n",
    "  \n",
    "- The encoded tokens are passed through the `model` (e.g., BERT) to compute embeddings, where the `[CLS]` token representation is extracted.\n",
    "\n",
    "- The function returns the `[CLS]` token embeddings, which capture contextual information of the entire input text in a fixed-size vector format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0474e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two names\n",
    "def name_similarity(name1, name2):\n",
    "    #Encode names\n",
    "    embedding1 = encode(name1)\n",
    "    embedding2 = encode(name2)\n",
    "    #flatten the embeddings to 1D\n",
    "    embedding1 = embedding1.squeeze().numpy()\n",
    "    embedding2 = embedding2.squeeze().numpy()\n",
    "    #Calculate cosine similarity\n",
    "    similarity = 1 - cosine(embedding1, embedding2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec93ea3e",
   "metadata": {},
   "source": [
    "This function `name_similarity` calculates the cosine similarity between two names represented as embeddings. It encodes each name into a numerical embedding, flattens the embeddings to 1D arrays, then computes the cosine similarity between these arrays. The resulting similarity score ranges from -1 (dissimilar) to 1 (identical), where higher values indicate greater similarity between the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e43bdddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between 'Adel hany' and 'Adel Hany' is: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Example names\n",
    "name1 = \"Adel hany\"\n",
    "name2 = \"Adel Hany\"\n",
    "\n",
    "# Compute similarity\n",
    "similarity = name_similarity(name1, name2)\n",
    "print(f\"The similarity between '{name1}' and '{name2}' is: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63746bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between 'Adel hany' and 'Ahmed Ali' is: 0.9177\n"
     ]
    }
   ],
   "source": [
    "# Example names\n",
    "name1 = \"Adel hany\"\n",
    "name2 = \"Ahmed Ali\"\n",
    "\n",
    "# Compute similarity\n",
    "similarity = name_similarity(name1, name2)\n",
    "print(f\"The similarity between '{name1}' and '{name2}' is: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776baae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
